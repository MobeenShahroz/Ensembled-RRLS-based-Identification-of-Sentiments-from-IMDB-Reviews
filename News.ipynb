{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3Va3r1RP4R3",
    "outputId": "8c6bb499-5171-4866-e189-47b971291c06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Accuracy: 0.9228729265314632 ± 0.00972392455109911\n",
      "Precision: 0.9246476512023005 ± 0.008516217663304279\n",
      "Recall: 0.9228729265314632 ± 0.00972392455109911\n",
      "F1-Score: 0.9220699307299496 ± 0.010126001893883244\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import re\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_excel(\"news.xlsx\", engine=\"openpyxl\")\n",
    "data['news'] = data['news'].apply(preprocess_text)\n",
    "X = data['news']\n",
    "y = data['label'].astype('category').cat.codes  # Encode labels\n",
    "\n",
    "# Convert text to feature vectors using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores, precision_scores, recall_scores, f1_scores = [], [], [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_tfidf, y):\n",
    "    X_train, X_test = X_tfidf[train_idx], X_tfidf[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Base models\n",
    "    rf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "    lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    svm = SVC(probability=True, random_state=42)\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "    lr.fit(X_train, y_train)\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    rf_preds = rf.predict_proba(X_train)\n",
    "    lr_preds = lr.predict_proba(X_train)\n",
    "    svm_preds = svm.predict_proba(X_train)\n",
    "    stacked_features = np.hstack((rf_preds, lr_preds, svm_preds))\n",
    "\n",
    "    # Meta-model\n",
    "    meta_model = Sequential()\n",
    "    meta_model.add(Dense(128, activation='relu', input_dim=stacked_features.shape[1]))\n",
    "    meta_model.add(Dropout(0.5))\n",
    "    meta_model.add(Dense(64, activation='relu'))\n",
    "    meta_model.add(Dropout(0.5))\n",
    "    meta_model.add(Dense(len(y.unique()), activation='softmax'))\n",
    "    meta_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    meta_model.fit(stacked_features, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    # Test predictions\n",
    "    rf_test_preds = rf.predict_proba(X_test)\n",
    "    lr_test_preds = lr.predict_proba(X_test)\n",
    "    svm_test_preds = svm.predict_proba(X_test)\n",
    "    stacked_test_features = np.hstack((rf_test_preds, lr_test_preds, svm_test_preds))\n",
    "    meta_test_preds = np.argmax(meta_model.predict(stacked_test_features), axis=1)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy_scores.append(accuracy_score(y_test, meta_test_preds))\n",
    "    precision_scores.append(precision_score(y_test, meta_test_preds, average='weighted'))\n",
    "    recall_scores.append(recall_score(y_test, meta_test_preds, average='weighted'))\n",
    "    f1_scores.append(f1_score(y_test, meta_test_preds, average='weighted'))\n",
    "\n",
    "# Print average performance\n",
    "print(f\"Accuracy: {np.mean(accuracy_scores)} ± {np.std(accuracy_scores)}\")\n",
    "print(f\"Precision: {np.mean(precision_scores)} ± {np.std(precision_scores)}\")\n",
    "print(f\"Recall: {np.mean(recall_scores)} ± {np.std(recall_scores)}\")\n",
    "print(f\"F1-Score: {np.mean(f1_scores)} ± {np.std(f1_scores)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mobee\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mobee\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\mobee\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Accuracy: 0.9277524643378303 ± 0.003116180907960876\n",
      "Precision: 0.9287531729673615 ± 0.0030931157554715764\n",
      "Recall: 0.9277524643378303 ± 0.003116180907960876\n",
      "F1-Score: 0.9272091280730022 ± 0.0032341651889648197\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import re\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_excel(\"news.xlsx\", engine=\"openpyxl\")\n",
    "data['news'] = data['news'].apply(preprocess_text)\n",
    "X = data['news']\n",
    "y = data['label'].astype('category').cat.codes  # Encode labels\n",
    "\n",
    "# Convert text to feature vectors using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores, precision_scores, recall_scores, f1_scores = [], [], [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_tfidf, y):\n",
    "    X_train, X_test = X_tfidf[train_idx], X_tfidf[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Apply SMOTE to balance classes\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Base models\n",
    "    rf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "    lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    svm = SVC(probability=True, random_state=42)\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "    lr.fit(X_train, y_train)\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    rf_preds = rf.predict_proba(X_train)\n",
    "    lr_preds = lr.predict_proba(X_train)\n",
    "    svm_preds = svm.predict_proba(X_train)\n",
    "    stacked_features = np.hstack((rf_preds, lr_preds, svm_preds))\n",
    "\n",
    "    # Meta-model\n",
    "    meta_model = Sequential()\n",
    "    meta_model.add(Dense(128, activation='relu', input_dim=stacked_features.shape[1]))\n",
    "    meta_model.add(Dropout(0.5))\n",
    "    meta_model.add(Dense(64, activation='relu'))\n",
    "    meta_model.add(Dropout(0.5))\n",
    "    meta_model.add(Dense(len(np.unique(y)), activation='softmax'))  # Number of unique classes\n",
    "    meta_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    meta_model.fit(stacked_features, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    # Test predictions\n",
    "    rf_test_preds = rf.predict_proba(X_test)\n",
    "    lr_test_preds = lr.predict_proba(X_test)\n",
    "    svm_test_preds = svm.predict_proba(X_test)\n",
    "    stacked_test_features = np.hstack((rf_test_preds, lr_test_preds, svm_test_preds))\n",
    "    meta_test_preds = np.argmax(meta_model.predict(stacked_test_features), axis=1)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy_scores.append(accuracy_score(y_test, meta_test_preds))\n",
    "    precision_scores.append(precision_score(y_test, meta_test_preds, average='weighted'))\n",
    "    recall_scores.append(recall_score(y_test, meta_test_preds, average='weighted'))\n",
    "    f1_scores.append(f1_score(y_test, meta_test_preds, average='weighted'))\n",
    "\n",
    "# Print average performance\n",
    "print(f\"Accuracy: {np.mean(accuracy_scores)} ± {np.std(accuracy_scores)}\")\n",
    "print(f\"Precision: {np.mean(precision_scores)} ± {np.std(precision_scores)}\")\n",
    "print(f\"Recall: {np.mean(recall_scores)} ± {np.std(recall_scores)}\")\n",
    "print(f\"F1-Score: {np.mean(f1_scores)} ± {np.std(f1_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Stacked Model Accuracy: 0.9228702462848805 ± 0.0038164461698806432\n",
      "Baseline Model Accuracy: 0.860385359897555 ± 0.007740764963702081\n",
      "T-Test Results: t-statistic = 17.628660170222233, p-value = 6.081552661174083e-05\n",
      "The difference in performance is statistically significant (p < 0.05).\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "stacked_accuracy_scores, baseline_accuracy_scores = [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_tfidf, y):\n",
    "    X_train, X_test = X_tfidf[train_idx], X_tfidf[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Apply SMOTE to balance classes\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Base models\n",
    "    rf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "    lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    svm = SVC(probability=True, random_state=42)\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "    lr.fit(X_train, y_train)\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # Stacked model predictions\n",
    "    rf_preds = rf.predict_proba(X_train)\n",
    "    lr_preds = lr.predict_proba(X_train)\n",
    "    svm_preds = svm.predict_proba(X_train)\n",
    "    stacked_features = np.hstack((rf_preds, lr_preds, svm_preds))\n",
    "\n",
    "    # Meta-model\n",
    "    meta_model = Sequential()\n",
    "    meta_model.add(Dense(128, activation='relu', input_dim=stacked_features.shape[1]))\n",
    "    meta_model.add(Dropout(0.5))\n",
    "    meta_model.add(Dense(64, activation='relu'))\n",
    "    meta_model.add(Dropout(0.5))\n",
    "    meta_model.add(Dense(len(np.unique(y)), activation='softmax'))\n",
    "    meta_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    meta_model.fit(stacked_features, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    # Test predictions for stacked model\n",
    "    rf_test_preds = rf.predict_proba(X_test)\n",
    "    lr_test_preds = lr.predict_proba(X_test)\n",
    "    svm_test_preds = svm.predict_proba(X_test)\n",
    "    stacked_test_features = np.hstack((rf_test_preds, lr_test_preds, svm_test_preds))\n",
    "    meta_test_preds = np.argmax(meta_model.predict(stacked_test_features), axis=1)\n",
    "\n",
    "    # Test predictions for baseline model (e.g., Random Forest)\n",
    "    baseline_preds = rf.predict(X_test)\n",
    "\n",
    "    # Evaluate stacked model\n",
    "    stacked_accuracy_scores.append(accuracy_score(y_test, meta_test_preds))\n",
    "\n",
    "    # Evaluate baseline model\n",
    "    baseline_accuracy_scores.append(accuracy_score(y_test, baseline_preds))\n",
    "\n",
    "# Perform T-Test\n",
    "t_statistic, p_value = ttest_rel(stacked_accuracy_scores, baseline_accuracy_scores)\n",
    "\n",
    "# Print results\n",
    "print(f\"Stacked Model Accuracy: {np.mean(stacked_accuracy_scores)} ± {np.std(stacked_accuracy_scores)}\")\n",
    "print(f\"Baseline Model Accuracy: {np.mean(baseline_accuracy_scores)} ± {np.std(baseline_accuracy_scores)}\")\n",
    "print(f\"T-Test Results: t-statistic = {t_statistic}, p-value = {p_value}\")\n",
    "\n",
    "# Interpret p-value\n",
    "if p_value < 0.05:\n",
    "    print(\"The difference in performance is statistically significant (p < 0.05).\")\n",
    "else:\n",
    "    print(\"The difference in performance is not statistically significant (p >= 0.05).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from urduhack.preprocessing import normalize_whitespace, remove_punctuation\n",
    "from urduhack.tokenization import sentence_tokenizer\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import re\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_excel(\"news.xlsx\", engine=\"openpyxl\")\n",
    "data['news'] = data['news'].apply(preprocess_text)\n",
    "X = data['news']\n",
    "y = data['label'].astype('category').cat.codes  # Encode labels\n",
    "\n",
    "# Convert text to feature vectors using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(X).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6683362707859133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics - Accuracy: 0.5670731707317073, Precision: 0.0, Recall: 0.0, F1-score: 0.0\n",
      "Epoch 2, Loss: 0.6104997665416904\n",
      "Validation Metrics - Accuracy: 0.7548780487804878, Precision: 0.808, Recall: 0.5690140845070423, F1-score: 0.6677685950413224\n",
      "Epoch 3, Loss: 0.47376740102360887\n",
      "Validation Metrics - Accuracy: 0.7804878048780488, Precision: 0.8205128205128205, Recall: 0.6309859154929578, F1-score: 0.7133757961783439\n",
      "Fold 1 Metrics:\n",
      "Accuracy: 0.7804878048780488\n",
      "Precision: 0.8205128205128205\n",
      "Recall: 0.6309859154929578\n",
      "F1-score: 0.7133757961783439\n",
      "--------------------------------------------------\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6784922625960373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics - Accuracy: 0.6134146341463415, Precision: 0.0, Recall: 0.0, F1-score: 0.0\n",
      "Epoch 2, Loss: 0.6698271005618863\n",
      "Validation Metrics - Accuracy: 0.7170731707317073, Precision: 0.7607361963190185, Recall: 0.3911671924290221, F1-score: 0.5166666666666667\n",
      "Epoch 3, Loss: 0.5577212550291201\n",
      "Validation Metrics - Accuracy: 0.7597560975609756, Precision: 0.718978102189781, Recall: 0.6214511041009464, F1-score: 0.6666666666666666\n",
      "Fold 2 Metrics:\n",
      "Accuracy: 0.7597560975609756\n",
      "Precision: 0.718978102189781\n",
      "Recall: 0.6214511041009464\n",
      "F1-score: 0.6666666666666666\n",
      "--------------------------------------------------\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6674401544943088\n",
      "Validation Metrics - Accuracy: 0.6605616605616605, Precision: 0.75, Recall: 0.22085889570552147, F1-score: 0.3412322274881517\n",
      "Epoch 2, Loss: 0.5694636528084918\n",
      "Validation Metrics - Accuracy: 0.724053724053724, Precision: 0.6893939393939394, Recall: 0.558282208588957, F1-score: 0.6169491525423729\n",
      "Epoch 3, Loss: 0.4749952661554988\n",
      "Validation Metrics - Accuracy: 0.7912087912087912, Precision: 0.739938080495356, Recall: 0.7331288343558282, F1-score: 0.736517719568567\n",
      "Fold 3 Metrics:\n",
      "Accuracy: 0.7912087912087912\n",
      "Precision: 0.739938080495356\n",
      "Recall: 0.7331288343558282\n",
      "F1-score: 0.736517719568567\n",
      "--------------------------------------------------\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6612022975595986\n",
      "Validation Metrics - Accuracy: 0.6251526251526252, Precision: 0.522633744855967, Recall: 0.7720364741641338, F1-score: 0.6233128834355828\n",
      "Epoch 2, Loss: 0.5524608352562276\n",
      "Validation Metrics - Accuracy: 0.7655677655677655, Precision: 0.8309178743961353, Recall: 0.5227963525835866, F1-score: 0.6417910447761194\n",
      "Epoch 3, Loss: 0.4472556909409965\n",
      "Validation Metrics - Accuracy: 0.7716727716727717, Precision: 0.669047619047619, Recall: 0.8541033434650456, F1-score: 0.7503337783711616\n",
      "Fold 4 Metrics:\n",
      "Accuracy: 0.7716727716727717\n",
      "Precision: 0.669047619047619\n",
      "Recall: 0.8541033434650456\n",
      "F1-score: 0.7503337783711616\n",
      "--------------------------------------------------\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mobee\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6623092508897549\n",
      "Validation Metrics - Accuracy: 0.6910866910866911, Precision: 0.5833333333333334, Recall: 0.6888888888888889, F1-score: 0.6317321688500728\n",
      "Epoch 2, Loss: 0.5330579762051745\n",
      "Validation Metrics - Accuracy: 0.7838827838827839, Precision: 0.7899159663865546, Recall: 0.5968253968253968, F1-score: 0.6799276672694394\n",
      "Epoch 3, Loss: 0.4338718640004716\n",
      "Validation Metrics - Accuracy: 0.778998778998779, Precision: 0.7278911564625851, Recall: 0.6793650793650794, F1-score: 0.7027914614121511\n",
      "Fold 5 Metrics:\n",
      "Accuracy: 0.778998778998779\n",
      "Precision: 0.7278911564625851\n",
      "Recall: 0.6793650793650794\n",
      "F1-score: 0.7027914614121511\n",
      "--------------------------------------------------\n",
      "\n",
      "Results Table:\n",
      "   Fold  Accuracy  Precision   Recall  F1-score\n",
      "      1  0.780488   0.820513 0.630986  0.713376\n",
      "      2  0.759756   0.718978 0.621451  0.666667\n",
      "      3  0.791209   0.739938 0.733129  0.736518\n",
      "      4  0.771673   0.669048 0.854103  0.750334\n",
      "      5  0.778999   0.727891 0.679365  0.702791\n",
      "Average  0.776425   0.735274 0.703807  0.713937\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch import nn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Enable multi-processing for tokenization\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text.lower()  # Convert to lowercase\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_excel(\"news.xlsx\", engine=\"openpyxl\")\n",
    "data['news'] = data['news'].apply(preprocess_text)\n",
    "X = data['news']\n",
    "y = data['label'].astype('category').cat.codes  # Encode labels\n",
    "\n",
    "# Convert text to feature vectors using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(X).toarray()\n",
    "X = pd.DataFrame(X)  # Convert to DataFrame to avoid shape issues\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_text(texts, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),  # Convert pandas Series to list\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)  # Convert labels to tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, optimizer, device, epochs=3):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(val_labels, val_preds)\n",
    "        precision = precision_score(val_labels, val_preds, average='binary')  # Change to 'macro' for multiclass\n",
    "        recall = recall_score(val_labels, val_preds, average='binary')\n",
    "        f1 = f1_score(val_labels, val_preds, average='binary')\n",
    "\n",
    "        print(f\"Validation Metrics - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}\")\n",
    "\n",
    "    return val_preds, val_labels\n",
    "\n",
    "# Initialize K-Fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Store evaluation metrics for each fold\n",
    "results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # Convert feature vectors back to text (if needed)\n",
    "    X_train_text = data.iloc[train_idx]['news']\n",
    "    X_val_text = data.iloc[val_idx]['news']\n",
    "\n",
    "    # Tokenize the training and validation data\n",
    "    X_train_tokens = tokenize_text(X_train_text)\n",
    "    X_val_tokens = tokenize_text(X_val_text)\n",
    "\n",
    "    # Create custom datasets\n",
    "    train_dataset = CustomDataset(X_train_tokens, y_train.tolist())\n",
    "    val_dataset = CustomDataset(X_val_tokens, y_val.tolist())\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "    # Load pre-trained BERT model for sequence classification\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(y.unique()))\n",
    "\n",
    "    # Set up optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    val_preds, val_labels = train_model(model, train_loader, val_loader, optimizer, device)\n",
    "\n",
    "    # Calculate metrics for the current fold\n",
    "    accuracy = accuracy_score(val_labels, val_preds)\n",
    "    precision = precision_score(val_labels, val_preds, average='binary')\n",
    "    recall = recall_score(val_labels, val_preds, average='binary')\n",
    "    f1 = f1_score(val_labels, val_preds, average='binary')\n",
    "\n",
    "    # Store metrics for the current fold\n",
    "    results.append({\n",
    "        'Fold': fold + 1,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-score': f1\n",
    "    })\n",
    "\n",
    "    print(f\"Fold {fold + 1} Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-score: {f1}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate average metrics across all folds\n",
    "avg_metrics = pd.DataFrame({\n",
    "    'Fold': ['Average'],\n",
    "    'Accuracy': [results_df['Accuracy'].mean()],\n",
    "    'Precision': [results_df['Precision'].mean()],\n",
    "    'Recall': [results_df['Recall'].mean()],\n",
    "    'F1-score': [results_df['F1-score'].mean()]\n",
    "})\n",
    "\n",
    "# Append the average metrics to the results table\n",
    "results_df = pd.concat([results_df, avg_metrics], ignore_index=True)\n",
    "\n",
    "# Print the results table\n",
    "print(\"\\nResults Table:\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating xlm-roberta-base...\n",
      "\n",
      "Fold 1 for xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5101673165472542\n",
      "Validation Metrics - Accuracy: 0.8768292682926829, Precision: 0.8175, Recall: 0.9211267605633803, F1-score: 0.866225165562914\n",
      "Epoch 2, Loss: 0.25060298875337694\n",
      "Validation Metrics - Accuracy: 0.9109756097560976, Precision: 0.8615384615384616, Recall: 0.9464788732394366, F1-score: 0.9020134228187919\n",
      "Epoch 3, Loss: 0.1453780756672708\n",
      "Validation Metrics - Accuracy: 0.9512195121951219, Precision: 0.972972972972973, Recall: 0.9126760563380282, F1-score: 0.9418604651162791\n",
      "Fold 1 Metrics for xlm-roberta-base:\n",
      "Accuracy: 0.9512195121951219\n",
      "Precision: 0.972972972972973\n",
      "Recall: 0.9126760563380282\n",
      "F1-score: 0.9418604651162791\n",
      "--------------------------------------------------\n",
      "\n",
      "Fold 2 for xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.49591529223977066\n",
      "Validation Metrics - Accuracy: 0.8975609756097561, Precision: 0.8498498498498499, Recall: 0.8927444794952681, F1-score: 0.8707692307692307\n",
      "Epoch 2, Loss: 0.25000100677333226\n",
      "Validation Metrics - Accuracy: 0.9390243902439024, Precision: 0.9211356466876972, Recall: 0.9211356466876972, F1-score: 0.9211356466876972\n",
      "Epoch 3, Loss: 0.13322081134722727\n",
      "Validation Metrics - Accuracy: 0.9451219512195121, Precision: 0.9, Recall: 0.9652996845425867, F1-score: 0.9315068493150684\n",
      "Fold 2 Metrics for xlm-roberta-base:\n",
      "Accuracy: 0.9451219512195121\n",
      "Precision: 0.9\n",
      "Recall: 0.9652996845425867\n",
      "F1-score: 0.9315068493150684\n",
      "--------------------------------------------------\n",
      "\n",
      "Fold 3 for xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.48300543932653056\n",
      "Validation Metrics - Accuracy: 0.8363858363858364, Precision: 0.8934426229508197, Recall: 0.6687116564417178, F1-score: 0.7649122807017544\n",
      "Epoch 2, Loss: 0.2610171414548304\n",
      "Validation Metrics - Accuracy: 0.9401709401709402, Precision: 0.9085545722713865, Recall: 0.9447852760736196, F1-score: 0.9263157894736842\n",
      "Epoch 3, Loss: 0.15289165747647243\n",
      "Validation Metrics - Accuracy: 0.9365079365079365, Precision: 0.8763736263736264, Recall: 0.9785276073619632, F1-score: 0.9246376811594202\n",
      "Fold 3 Metrics for xlm-roberta-base:\n",
      "Accuracy: 0.9365079365079365\n",
      "Precision: 0.8763736263736264\n",
      "Recall: 0.9785276073619632\n",
      "F1-score: 0.9246376811594202\n",
      "--------------------------------------------------\n",
      "\n",
      "Fold 4 for xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.44178742893585343\n",
      "Validation Metrics - Accuracy: 0.9096459096459096, Precision: 0.9381443298969072, Recall: 0.8297872340425532, F1-score: 0.8806451612903226\n",
      "Epoch 2, Loss: 0.27564226517044915\n",
      "Validation Metrics - Accuracy: 0.8986568986568987, Precision: 0.8075, Recall: 0.9817629179331308, F1-score: 0.8861454046639232\n",
      "Epoch 3, Loss: 0.1318964233122221\n",
      "Validation Metrics - Accuracy: 0.9181929181929182, Precision: 0.8341836734693877, Recall: 0.993920972644377, F1-score: 0.9070735090152566\n",
      "Fold 4 Metrics for xlm-roberta-base:\n",
      "Accuracy: 0.9181929181929182\n",
      "Precision: 0.8341836734693877\n",
      "Recall: 0.993920972644377\n",
      "F1-score: 0.9070735090152566\n",
      "--------------------------------------------------\n",
      "\n",
      "Fold 5 for xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5293294324380595\n",
      "Validation Metrics - Accuracy: 0.8266178266178266, Precision: 0.7235142118863049, Recall: 0.8888888888888888, F1-score: 0.7977207977207977\n",
      "Epoch 2, Loss: 0.3015527929838111\n",
      "Validation Metrics - Accuracy: 0.8937728937728938, Precision: 0.7984293193717278, Recall: 0.9682539682539683, F1-score: 0.8751793400286944\n",
      "Epoch 3, Loss: 0.1673616782739395\n",
      "Validation Metrics - Accuracy: 0.9316239316239316, Precision: 0.884272997032641, Recall: 0.946031746031746, F1-score: 0.9141104294478528\n",
      "Fold 5 Metrics for xlm-roberta-base:\n",
      "Accuracy: 0.9316239316239316\n",
      "Precision: 0.884272997032641\n",
      "Recall: 0.946031746031746\n",
      "F1-score: 0.9141104294478528\n",
      "--------------------------------------------------\n",
      "\n",
      "Evaluating roberta-base...\n",
      "\n",
      "Fold 1 for roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5277713326419272\n",
      "Validation Metrics - Accuracy: 0.7792682926829269, Precision: 0.967741935483871, Recall: 0.5070422535211268, F1-score: 0.6654343807763401\n",
      "Epoch 2, Loss: 0.40688933599285965\n",
      "Validation Metrics - Accuracy: 0.8695121951219512, Precision: 0.8734939759036144, Recall: 0.8169014084507042, F1-score: 0.8442503639010189\n",
      "Epoch 3, Loss: 0.3552560709598588\n",
      "Validation Metrics - Accuracy: 0.8512195121951219, Precision: 0.8319088319088319, Recall: 0.8225352112676056, F1-score: 0.8271954674220963\n",
      "Fold 1 Metrics for roberta-base:\n",
      "Accuracy: 0.8512195121951219\n",
      "Precision: 0.8319088319088319\n",
      "Recall: 0.8225352112676056\n",
      "F1-score: 0.8271954674220963\n",
      "--------------------------------------------------\n",
      "\n",
      "Fold 2 for roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5486115062382163\n",
      "Validation Metrics - Accuracy: 0.7878048780487805, Precision: 0.9863945578231292, Recall: 0.45741324921135645, F1-score: 0.625\n",
      "Epoch 2, Loss: 0.42562918139667044\n",
      "Validation Metrics - Accuracy: 0.8121951219512196, Precision: 0.8438818565400844, Recall: 0.6309148264984227, F1-score: 0.7220216606498195\n",
      "Epoch 3, Loss: 0.41246494534539013\n",
      "Validation Metrics - Accuracy: 0.8341463414634146, Precision: 0.9371980676328503, Recall: 0.61198738170347, F1-score: 0.7404580152671756\n",
      "Fold 2 Metrics for roberta-base:\n",
      "Accuracy: 0.8341463414634146\n",
      "Precision: 0.9371980676328503\n",
      "Recall: 0.61198738170347\n",
      "F1-score: 0.7404580152671756\n",
      "--------------------------------------------------\n",
      "\n",
      "Fold 3 for roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5456773738308651\n",
      "Validation Metrics - Accuracy: 0.8315018315018315, Precision: 0.8615384615384616, Recall: 0.6871165644171779, F1-score: 0.764505119453925\n",
      "Epoch 2, Loss: 0.40438864472799185\n",
      "Validation Metrics - Accuracy: 0.8534798534798534, Precision: 0.9401709401709402, Recall: 0.6748466257668712, F1-score: 0.7857142857142857\n",
      "Epoch 3, Loss: 0.362359304871501\n",
      "Validation Metrics - Accuracy: 0.8559218559218559, Precision: 0.8795620437956204, Recall: 0.7392638036809815, F1-score: 0.8033333333333333\n",
      "Fold 3 Metrics for roberta-base:\n",
      "Accuracy: 0.8559218559218559\n",
      "Precision: 0.8795620437956204\n",
      "Recall: 0.7392638036809815\n",
      "F1-score: 0.8033333333333333\n",
      "--------------------------------------------------\n",
      "\n",
      "Fold 4 for roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4754488765829947\n",
      "Validation Metrics - Accuracy: 0.833943833943834, Precision: 0.8163934426229508, Recall: 0.756838905775076, F1-score: 0.7854889589905363\n",
      "Epoch 2, Loss: 0.38425289476063196\n",
      "Validation Metrics - Accuracy: 0.8315018315018315, Precision: 0.799373040752351, Recall: 0.7750759878419453, F1-score: 0.7870370370370371\n",
      "Epoch 3, Loss: 0.3477637877369799\n",
      "Validation Metrics - Accuracy: 0.851037851037851, Precision: 0.8327974276527331, Recall: 0.7872340425531915, F1-score: 0.809375\n",
      "Fold 4 Metrics for roberta-base:\n",
      "Accuracy: 0.851037851037851\n",
      "Precision: 0.8327974276527331\n",
      "Recall: 0.7872340425531915\n",
      "F1-score: 0.809375\n",
      "--------------------------------------------------\n",
      "\n",
      "Fold 5 for roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5554673905779676\n",
      "Validation Metrics - Accuracy: 0.8070818070818071, Precision: 0.7813620071684588, Recall: 0.692063492063492, F1-score: 0.734006734006734\n",
      "Epoch 2, Loss: 0.48751439190492396\n",
      "Validation Metrics - Accuracy: 0.800976800976801, Precision: 1.0, Recall: 0.48253968253968255, F1-score: 0.6509635974304069\n",
      "Epoch 3, Loss: 0.47423879831302457\n",
      "Validation Metrics - Accuracy: 0.7985347985347986, Precision: 0.987012987012987, Recall: 0.48253968253968255, F1-score: 0.6481876332622601\n",
      "Fold 5 Metrics for roberta-base:\n",
      "Accuracy: 0.7985347985347986\n",
      "Precision: 0.987012987012987\n",
      "Recall: 0.48253968253968255\n",
      "F1-score: 0.6481876332622601\n",
      "--------------------------------------------------\n",
      "\n",
      "Results for XLMRoberta:\n",
      "   Fold  Accuracy  Precision   Recall  F1-score\n",
      "      1  0.951220   0.972973 0.912676  0.941860\n",
      "      2  0.945122   0.900000 0.965300  0.931507\n",
      "      3  0.936508   0.876374 0.978528  0.924638\n",
      "      4  0.918193   0.834184 0.993921  0.907074\n",
      "      5  0.931624   0.884273 0.946032  0.914110\n",
      "Average  0.936533   0.893561 0.959291  0.923838\n",
      "\n",
      "Results for Roberta:\n",
      "   Fold  Accuracy  Precision   Recall  F1-score\n",
      "      1  0.851220   0.831909 0.822535  0.827195\n",
      "      2  0.834146   0.937198 0.611987  0.740458\n",
      "      3  0.855922   0.879562 0.739264  0.803333\n",
      "      4  0.851038   0.832797 0.787234  0.809375\n",
      "      5  0.798535   0.987013 0.482540  0.648188\n",
      "Average  0.838172   0.893696 0.688712  0.765710\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import (\n",
    "    XLMRobertaTokenizer, XLMRobertaForSequenceClassification,\n",
    "    RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_excel(\"news.xlsx\", engine=\"openpyxl\")\n",
    "data['news'] = data['news'].apply(preprocess_text)\n",
    "X = data['news']\n",
    "y = data['label'].astype('category').cat.codes  # Encode labels\n",
    "\n",
    "# Enable multi-processing\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# Initialize K-Fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store evaluation metrics for each fold\n",
    "results_xlm_roberta = []\n",
    "results_roberta = []\n",
    "\n",
    "# Custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, optimizer, device, epochs=3):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(val_labels, val_preds)\n",
    "        precision = precision_score(val_labels, val_preds, average='binary')\n",
    "        recall = recall_score(val_labels, val_preds, average='binary')\n",
    "        f1 = f1_score(val_labels, val_preds, average='binary')\n",
    "\n",
    "        print(f\"Validation Metrics - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}\")\n",
    "\n",
    "    return val_preds, val_labels\n",
    "\n",
    "# Function to evaluate a model\n",
    "def evaluate_model(model_name, tokenizer_class, model_class, results):\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "        print(f\"\\nFold {fold + 1} for {model_name}\")\n",
    "\n",
    "        # Split data into training and validation sets\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Tokenize the training and validation data\n",
    "        X_train_tokens = tokenizer(\n",
    "            X_train.tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt'\n",
    "        )\n",
    "        X_val_tokens = tokenizer(\n",
    "            X_val.tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Create custom datasets\n",
    "        train_dataset = CustomDataset(X_train_tokens, y_train.tolist())\n",
    "        val_dataset = CustomDataset(X_val_tokens, y_val.tolist())\n",
    "\n",
    "        # Create DataLoader\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "        # Load pre-trained model for sequence classification\n",
    "        model = model_class.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "        # Set up optimizer\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        val_preds, val_labels = train_model(model, train_loader, val_loader, optimizer, device)\n",
    "\n",
    "        # Calculate metrics for the current fold\n",
    "        accuracy = accuracy_score(val_labels, val_preds)\n",
    "        precision = precision_score(val_labels, val_preds, average='binary')\n",
    "        recall = recall_score(val_labels, val_preds, average='binary')\n",
    "        f1 = f1_score(val_labels, val_preds, average='binary')\n",
    "\n",
    "        # Store metrics for the current fold\n",
    "        results.append({\n",
    "            'Fold': fold + 1,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1\n",
    "        })\n",
    "\n",
    "        print(f\"Fold {fold + 1} Metrics for {model_name}:\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1-score: {f1}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Evaluate XLMRoberta\n",
    "results_xlm_roberta = evaluate_model(\n",
    "    \"xlm-roberta-base\",\n",
    "    XLMRobertaTokenizer,\n",
    "    XLMRobertaForSequenceClassification,\n",
    "    results_xlm_roberta\n",
    ")\n",
    "\n",
    "# Evaluate Roberta\n",
    "results_roberta = evaluate_model(\n",
    "    \"roberta-base\",\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    results_roberta\n",
    ")\n",
    "\n",
    "# Convert results to DataFrames\n",
    "results_xlm_roberta_df = pd.DataFrame(results_xlm_roberta)\n",
    "results_roberta_df = pd.DataFrame(results_roberta)\n",
    "\n",
    "# Calculate average metrics across all folds for each model\n",
    "avg_metrics_xlm_roberta = pd.DataFrame({\n",
    "    'Fold': ['Average'],\n",
    "    'Accuracy': [results_xlm_roberta_df['Accuracy'].mean()],\n",
    "    'Precision': [results_xlm_roberta_df['Precision'].mean()],\n",
    "    'Recall': [results_xlm_roberta_df['Recall'].mean()],\n",
    "    'F1-score': [results_xlm_roberta_df['F1-score'].mean()]\n",
    "})\n",
    "\n",
    "avg_metrics_roberta = pd.DataFrame({\n",
    "    'Fold': ['Average'],\n",
    "    'Accuracy': [results_roberta_df['Accuracy'].mean()],\n",
    "    'Precision': [results_roberta_df['Precision'].mean()],\n",
    "    'Recall': [results_roberta_df['Recall'].mean()],\n",
    "    'F1-score': [results_roberta_df['F1-score'].mean()]\n",
    "})\n",
    "\n",
    "# Append the average metrics to the results tables\n",
    "results_xlm_roberta_df = pd.concat([results_xlm_roberta_df, avg_metrics_xlm_roberta], ignore_index=True)\n",
    "results_roberta_df = pd.concat([results_roberta_df, avg_metrics_roberta], ignore_index=True)\n",
    "\n",
    "# Print the results tables\n",
    "print(\"\\nResults for XLMRoberta:\")\n",
    "print(results_xlm_roberta_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nResults for Roberta:\")\n",
    "print(results_roberta_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
